{"cells":[{"cell_type":"markdown","metadata":{"id":"XD6ZS0eWbsnl"},"source":["# ch04/gradient_1d.py"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":21098,"status":"ok","timestamp":1709017836048,"user":{"displayName":"h지영","userId":"09931517472368185956"},"user_tz":-540},"id":"AD5WFabMBIZB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4e72283-b7ce-48a0-a94f-312a49541aaf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys, os\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/밑바닥부터시작하는')\n","sys.path.append(os.pardir)\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/밑바닥부터시작하는/ch04')\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/밑바닥부터시작하는/common')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDK9dFotao4W"},"outputs":[],"source":["## 손실함수 ##\n","\n","# #오차제곱합\n","# y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","# t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","\n","# def sum_squares_error(y, t):\n","#   return 0.5 * np.sum((y-t)**2)\n","\n","# t = [0, 0, 1, 0, 0, 0, 0, 0, 0]\n","\n","# #예1: '2'일 확률이 가장 높다고 추정함 (0.6)\n","# y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","# sum_squares_error(np.array(y), np.array(t)) #0.097500000000000031\n","\n","# #예2: '7'일 확률이 가장 높다고 추정함 (0.6)\n","# y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","# sum_squares_error(np.array(y), np.array(t)) #0.59750000000000003\n","\n","\n","# def cross_entropy_error(y, t):\n","#   delta = 1e-7 # 0일때 -무한대가 되지 않기 위해 작은 값을 더함\n","#   return -np.sum(t* np.log(y + delta))\n","\n","# t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n","# y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","# cross_entropy_error(np.array(y), np.array(t)) #0.51082545709933802\n","\n","# y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","# cross_entropy_error(np.array(y), np.array(t)) #2,3025840929945458\n","\n","# ## 미니배치 학습 ##\n","# import sys, os\n","# sys.path.append(os.pardir)\n","# import numpy as np\n","# from mnist import load_mnist\n","\n","# (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","# print(x_train.shape) # (60000, 784)\n","# print(t_train.shape) # (60000, 10)\n","\n","train_size = x_train.shape[0]\n","batch_size = 10\n","batch_mask = np.random.choice(train_size, batch_size) # 범위 중 무작위 10개 추출\n","x_batch = x_train[batch_mask]\n","t_batch = t_train[batch_mask]\n","\n","#np.random.choice(60000, 10) #array([ 3070, 49842, 13487, 11604, 50348, 17294,  5694, 35492, 56758,58402])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBIbuAr7bH0q"},"outputs":[],"source":["## (배치용) 교차 엔트로피 오차 구현하기 ##\n","def cross_entropy_error(y, t):\n","  if y.ndim == 1:\n","    t = t.reshape(1, t.size)\n","    y = y.reshape(1, y.size)\n","  batch_size = y.shape[0]\n","  return -np.sum(t* np.log(y+ 1e-7)) / batch_size\n","\n","def cross_entropy_error(y, t):\n","  if y.ndim == 1:\n","    t = t.reshape(1, t.size)\n","    y = y.reshape(1, y.size)\n","  batch_size = y.shape[0]\n","  return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0 ]\n","y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n","cross_entropy_error(np.array(y), np.array(t)) #0.510825457099338\n","y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n","cross_entropy_error(np.array(y), np.array(t)) #2.302584092994546"]},{"cell_type":"markdown","source":["수치미분"],"metadata":{"id":"yYnWHhWIW4pE"}},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":272,"status":"ok","timestamp":1709018117958,"user":{"displayName":"h지영","userId":"09931517472368185956"},"user_tz":-540},"id":"1W7a2SLbbsnn","outputId":"2e38a146-9bbf-4e4f-cba1-6a8d821627b4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.00000000000378"]},"metadata":{},"execution_count":5}],"source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","## 나쁜 구현 예 ##\n","def numerical_diff(f, x):\n","    h = 1e-50\n","    return (f(x + h) - f(x)) / h\n","\n","# 반올림 오차문제(너무작은값)\n","np.float32(1e-50) #0.0\n","\n","## 중심차분, 중앙차분 ##\n","def numerical_diff(f, x):\n","    h = 1e-4 # 0.0001\n","    return (f(x+h) - f(x-h)) / (2*h)\n","\n","\n","## 수치미분의 예 ##\n","def function_1(x):\n","    return 0.01*x**2 + 0.1*x\n","\n","#접선구하는 함수\n","def tangent_line(f, x):\n","    d = numerical_diff(f, x)\n","    print(d)\n","    y = f(x) - d*x\n","    return lambda t: d*t + y\n","\n","x = np.arange(0.0, 20.0, 0.1)  # 0에서 20까지 0.1 간격의 배열 X를 만든다(20은 미포함)\n","y = function_1(x)\n","# plt.xlabel(\"x\")\n","# plt.ylabel(\"f(x)\")\n","# plt.plot(x, y)\n","# plt.show()\n","\n","tf = tangent_line(function_1, 5)\n","y2 = tf(x)\n","\n","plt.plot(x, y)\n","plt.plot(x, y2)\n","plt.show()\n","\n","numerical_diff(function_1, 5)\n","numerical_diff(function_1, 10)\n","\n","## 편미분 ##\n","def function_2(x):\n","  return x[0]**2 + x[1]**2  #또는 return np.sum(x**2)\n","\n","# # Q1. x0 = 3, x1 = 4일때, x0에 대한  편미분\n","def function_tmp1(x0):\n","  return x0**2 + 4.0**2.0\n","\n","numerical_diff(function_tmp1, 3.0)  # 6.00000000000378\n","\n","# Q2. x0 = 3, x1 = 4일때, x1에 대한  편미분\n","# def function_tmp2(x1):\n","#   return 3.0**2.0 + x1*x1\n","\n","# numerical_diff(function_tmp2, 4.0) # 7.999999999999119"]},{"cell_type":"markdown","metadata":{"id":"VWjDRBxRbsnp"},"source":["# ch04/gradient_2d.py"]},{"cell_type":"markdown","metadata":{"id":"UaY5kx6THS8A"},"source":["기울기"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1709018575102,"user":{"displayName":"h지영","userId":"09931517472368185956"},"user_tz":-540},"id":"Ru3sW-2qbsnp","outputId":"ee64b459-5712-4583-f609-22ee3089dd57"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([6., 0.])"]},"metadata":{},"execution_count":7}],"source":["def numerical_gradient(f, x):\n","  h = 1e-4 # 0,0001\n","  grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n","  for idx in range(x.size):\n","    tmp_val = x[idx] # f(x+h)\n","    x[idx] = tmp_val + h\n","    fxh1= f(x)\n","    #f(x-h) 계산\n","    x[idx] = tmp_val - h\n","    fxh2 = f(x)\n","\n","    grad[idx] = (fxh1-fxh2) / (2*h)\n","    x[idx] = tmp_val #값 복원\n","  return grad\n","\n","numerical_gradient(function_2, np.array([3.0, 4.0]))\n","numerical_gradient(function_2, np.array([0.0, 2.0]))\n","numerical_gradient(function_2, np.array([3.0, 0.0])) #array([6., 0.])\n"]},{"cell_type":"markdown","metadata":{"id":"akVwXcNjbsnq"},"source":["# ch04/gradient_method.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xiHrfWCtbsnq"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pylab as plt\n","\n","## 경사 하강법 ##\n","def gradient_descent(f, init_x, lr=0.01, step_num=100):\n","    x = init_x\n","    x_history = []\n","\n","    for i in range(step_num):\n","        x_history.append( x.copy() )\n","\n","        grad = numerical_gradient(f, x)\n","        x -= lr * grad\n","\n","    return x, np.array(x_history)\n","\n","#Q1 : 경사법으로 f(xo,x1)=x0**2 + x1**2 의 최솟값을 구하라.\n","def function_2(x):\n","    return x[0]**2 + x[1]**2\n","\n","init_x = np.array([-3.0, 4.0])\n","gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100) #array([ -6,11110793e-10, 8.14814391e-10])\n","\n","\n","# #학습률이 너무 큰 예: 1r=10.0\n","# init_x = np.array([-3.0, 4.0])\n","# gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100) #array([ -2.58983747e+13, -1.29524862e+12])\n","\n","# #학습률이 너무 작은 예: lr=1e-10\n","# init_x = np.array([-3.0, 4.0])\n","# gradient_descent (function_2, init_x=init_x, lr=1e-10, step_num=100) #array([-2.99999994, 3.99999992])\n","\n","# # 그래프\n","# init_x = np.array([-3.0, 4.0])\n","# x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)\n","\n","# plt.plot([-5, 5], [0, 0], '--b')\n","# plt.plot([0, 0], [-5, 5], '--b')\n","# plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n","\n","# plt.xlim(-3.5, 3.5)\n","# plt.ylim(-4.5, 4.5)\n","# plt.xlabel(\"X0\")\n","# plt.ylabel(\"X1\")\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nR-1GF3Hbsnr"},"source":["# ch04/gradient_simplenet.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1801,"status":"ok","timestamp":1708929524604,"user":{"displayName":"h지영","userId":"09931517472368185956"},"user_tz":-540},"id":"JvgLeyyJbsnr","outputId":"12c4dea4-5499-4c08-f78e-348e2feacdf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.06858494  0.37487234 -0.44345728]\n"," [ 0.1028774   0.56230851 -0.66518592]]\n"]}],"source":["##신경망에서의 기울기 ##\n","import sys, os\n","sys.path.append(os.pardir)\n","import numpy as np\n","from common.functions import softmax, cross_entropy_error\n","from common.gradient import numerical_gradient\n","\n","\n","class simpleNet:\n","    def __init__(self):\n","        self.W = np.random.randn(2,3) #정규분포로 초기화\n","\n","    def predict(self, x):\n","        return np.dot(x, self.W)\n","\n","    def loss(self, x, t):\n","        z = self.predict(x)\n","        y = softmax(z)\n","        loss = cross_entropy_error(y, t)\n","\n","        return loss\n","\n","net = simpleNet()\n","#print(net.W) #가중치 매개변수 [[-1.02616074 -0.30301655 -0.63530905] [ 1.15123149 -0.53630072 -1.5930804 ]]\n","\n","x = np.array([0.6, 0.9])\n","#p = net.predict(x)\n","# print(p) #[ 0.04701656 -1.78292584  0.34509516]\n","#np.argmax(p) #최대값의 idx #0\n","\n","t = np.array([0, 0, 1]) #정답레이블\n","#net.loss(x, t) #2.1237823839422174\n","\n","def f(W):\n","  return net.loss(x,t)\n","dW = numerical_gradient(f, net.W)\n","print(dW) #[[ 0.45987683  0.06436874 -0.52424558] [ 0.68981525  0.09655312 -0.78636837]]\n","\n","# net = simpleNet()\n","#상위함수 lamda로\n","f = lambda w: net.loss(x, t)\n","dW = numerical_gradient(f, net.W)\n","\n","# print(dW)"]},{"cell_type":"markdown","metadata":{"id":"AZyC7hRSbsnr"},"source":["# ch04/two_layer_net.py"]},{"cell_type":"markdown","metadata":{"id":"W8kmGALVH_1B"},"source":["학습 알고리즘 구현하기\n","> 전제\n","  1단계-미니배치\n","  2단계-기울기 산출\n","  3단계-매개변수 갱신\n","  4단계 반복\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"PrX6ymXdbsnr","outputId":"0053eec2-2854-4a87-ed1c-02efe3af3035"},"outputs":[{"data":{"text/plain":["(10,)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["## 2층 신경망 Class 구현하기 ##\n","\n","import sys, os\n","sys.path.append(os.pardir)\n","from common.functions import *\n","from common.gradient import numerical_gradient\n","import numpy as np\n","\n","\n","class TwoLayerNet:\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n","        self.params['b2'] = np.zeros(output_size)\n","\n","    def predict(self, x):\n","        W1, W2 = self.params['W1'], self.params['W2']\n","        b1, b2 = self.params['b1'], self.params['b2']\n","\n","        a1 = np.dot(x, W1) + b1\n","        z1 = sigmoid(a1)\n","        a2 = np.dot(z1, W2) + b2\n","        y = softmax(a2)\n","\n","        return y\n","\n","    # 손실함수, x:입력데이터, t:정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","\n","        return cross_entropy_error(y, t)\n","\n","    #정확도\n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        t = np.argmax(t, axis=1)\n","\n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","\n","    # 가중치 매개변수의 기울기. x:입력데이터, t:정답레이블\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","\n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","\n","        return grads\n","\n","    # def gradient(self, x, t):\n","    #     W1, W2 = self.params['W1'], self.params['W2']\n","    #     b1, b2 = self.params['b1'], self.params['b2']\n","    #     grads = {}\n","\n","    #     batch_num = x.shape[0]\n","\n","    #     # forward\n","    #     a1 = np.dot(x, W1) + b1\n","    #     z1 = sigmoid(a1)\n","    #     a2 = np.dot(z1, W2) + b2\n","    #     y = softmax(a2)\n","\n","    #     # backward\n","    #     dy = (y - t) / batch_num\n","    #     grads['W2'] = np.dot(z1.T, dy)\n","    #     grads['b2'] = np.sum(dy, axis=0)\n","\n","    #     dz1 = np.dot(dy, W2.T)\n","    #     da1 = sigmoid_grad(a1) * dz1\n","    #     grads['W1'] = np.dot(x.T, da1)\n","    #     grads['b1'] = np.sum(da1, axis=0)\n","\n","    #     return grads\n","\n","#예\n","net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n","net.params['W1'].shape\n","net.params['b1'].shape\n","net.params['W2'].shape\n","net.params['b2'].shape #(10,)\n","\n","x = np.random.rand(100,784) #더미'입력'데이터(100장분량)\n","y = net.predict(x)\n","t= np.random.rand(100,10) #더미'정답'데이터(100장분량)\n","\n","grads = net.numerical_gradient(x,t) #기울기 계산 (오래걸림 주의)\n","net.params['W1'].shape\n","net.params['b1'].shape\n","net.params['W2'].shape\n","net.params['b2'].shape #(10,)\n"]},{"cell_type":"markdown","metadata":{"id":"DJtBTwN6bsns"},"source":["# ch04/train_neuralnet.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"elapsed":2461,"status":"error","timestamp":1708407598332,"user":{"displayName":"h지영","userId":"09931517472368185956"},"user_tz":-540},"id":"6IRtICzSbsns","outputId":"2af501a8-afdf-421d-e037-49aca0a545b7"},"outputs":[{"ename":"AttributeError","evalue":"'TwoLayerNet' object has no attribute 'gradient'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-35-21e7f4b16381>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# 기울기 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#grad = network.numerical_gradient(x_batch, t_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#성능 개선판!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# 매개변수 갱신\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'TwoLayerNet' object has no attribute 'gradient'"]}],"source":["#미니배치 학습 구현하기\n","import sys, os\n","sys.path.append(os.pardir)\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from dataset.mnist import load_mnist\n","from two_layer_net import TwoLayerNet\n","\n","\n","(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n","network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","# 하이퍼파라미터\n","iters_num = 10000  #반복 획수를 적절히 설정한다\n","train_size = x_train.shape[0]\n","batch_size = 100 #미니배치크기\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    #미니배치 획득\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = x_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","\n","    # 기울기 계산\n","    #grad = network.numerical_gradient(x_batch, t_batch)\n","    grad = network.gradient(x_batch, t_batch) #성능 개선판!\n","\n","    # 매개변수 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","\n","    # 학습 경과 기록\n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    # 1에폭당 정확도 계산\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(x_train, t_train)\n","        test_acc = network.accuracy(x_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"]},{"cell_type":"markdown","metadata":{"id":"XlN0BDfqbsnt"},"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dezero:Python","language":"python","name":"conda-env-dezero-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}